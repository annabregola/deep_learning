{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1bd859f",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "## Import of packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "450ca089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d06134e",
   "metadata": {},
   "source": [
    "## Organising the folders"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb1bbf85",
   "metadata": {},
   "source": [
    "!rm -rf '/home/labuser/all_data'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dbfe23bd",
   "metadata": {},
   "source": [
    "\n",
    "characters = ['bart_simpson', 'charles_montgomery_burns', 'homer_simpson', 'krusty_the_clown', \n",
    "              'lisa_simpson', 'marge_simpson', 'milhouse_van_houten', 'moe_szyslak', 'ned_flanders', 'principal_skinner']\n",
    "\n",
    "root_dir = \"/home/labuser/\"\n",
    "\n",
    "\n",
    "classes_dir = characters #total labels\n",
    "\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "\n",
    "for cls in classes_dir:\n",
    "   # Creating partitions of the data after shuffeling\n",
    "    src = root_dir + 'lab_01/' + cls # Folder to copy images from\n",
    "\n",
    "\n",
    "    os.makedirs(root_dir + 'all_data/' + 'train/' +  cls)\n",
    "    os.makedirs(root_dir + 'all_data/' + 'val/' + cls)\n",
    "    os.makedirs(root_dir + 'all_data/'+ 'test/'  + cls )\n",
    "\n",
    "\n",
    "    allFileNames = os.listdir(src)\n",
    "    np.random.shuffle(allFileNames)\n",
    "    train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),\n",
    "                                                              [int(len(allFileNames)* (1 - (val_ratio + test_ratio))), \n",
    "                                                              int(len(allFileNames)* (1 - test_ratio))])\n",
    "\n",
    "\n",
    "    train_FileNames = [src+'/'+ name for name in train_FileNames.tolist()]\n",
    "    val_FileNames = [src+'/' + name for name in val_FileNames.tolist()]\n",
    "    test_FileNames = [src+'/' + name for name in test_FileNames.tolist()]\n",
    "\n",
    "    # Copy-pasting images\n",
    "    for name in train_FileNames:\n",
    "        shutil.copyfile(name, '%s/%s' % (root_dir + 'all_data/' + 'train/'  + cls, name.split('/')[-1]))\n",
    "\n",
    "    for name in val_FileNames:\n",
    "        shutil.copyfile(name, '%s/%s' % (root_dir + 'all_data/'  + 'val/' + cls, name.split('/')[-1]))\n",
    "\n",
    "    for name in test_FileNames:\n",
    "        shutil.copyfile(name, '%s/%s' % (root_dir + 'all_data/' + 'test/'  + cls, name.split('/')[-1])) # data root path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "41aa03fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = {}\n",
    "\n",
    "characters = ['bart_simpson', 'charles_montgomery_burns', 'homer_simpson', 'krusty_the_clown', \n",
    "              'lisa_simpson', 'marge_simpson', 'milhouse_van_houten', 'moe_szyslak', 'ned_flanders', 'principal_skinner']\n",
    "base_dir = '/home/labuser/all_data/'\n",
    "for character in characters: \n",
    "      all_data[character] = {}\n",
    "      base_dir_ch = base_dir\n",
    "      all_data[character]['train'] =  os.path.join(base_dir_ch, 'train/' + character)\n",
    "      all_data[character]['test'] =  os.path.join(base_dir_ch, 'test/' + character)\n",
    "      all_data[character]['val'] =  os.path.join(base_dir_ch, 'val/' + character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7445afa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bart_simpson\n",
      "total train images: 1006\n",
      "total test images: 135\n",
      "total val images: 201\n",
      "--------------------\n",
      "charles_montgomery_burns\n",
      "total train images: 894\n",
      "total test images: 120\n",
      "total val images: 179\n",
      "--------------------\n",
      "homer_simpson\n",
      "total train images: 1684\n",
      "total test images: 225\n",
      "total val images: 337\n",
      "--------------------\n",
      "krusty_the_clown\n",
      "total train images: 904\n",
      "total test images: 121\n",
      "total val images: 181\n",
      "--------------------\n",
      "lisa_simpson\n",
      "total train images: 1015\n",
      "total test images: 136\n",
      "total val images: 203\n",
      "--------------------\n",
      "marge_simpson\n",
      "total train images: 968\n",
      "total test images: 130\n",
      "total val images: 193\n",
      "--------------------\n",
      "milhouse_van_houten\n",
      "total train images: 809\n",
      "total test images: 108\n",
      "total val images: 162\n",
      "--------------------\n",
      "moe_szyslak\n",
      "total train images: 1089\n",
      "total test images: 146\n",
      "total val images: 217\n",
      "--------------------\n",
      "ned_flanders\n",
      "total train images: 1090\n",
      "total test images: 146\n",
      "total val images: 218\n",
      "--------------------\n",
      "principal_skinner\n",
      "total train images: 895\n",
      "total test images: 120\n",
      "total val images: 179\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for ch in characters: \n",
    "      data = all_data[ch]\n",
    "      print(ch)\n",
    "      for k in data.keys(): \n",
    "          print(f'total {k} images:', len(os.listdir(data[k])))\n",
    "      print('-'*20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f153b31",
   "metadata": {},
   "source": [
    "## Building a CNN - Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ae8fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding = 'same',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5f041a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding = 'same',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.1))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.1))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.1))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.1))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c678b67",
   "metadata": {},
   "source": [
    "## Model Compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c5660ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(learning_rate=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bffc47a",
   "metadata": {},
   "source": [
    "## Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "215aca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale all images by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "58bd23b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/labuser/all_data/'\n",
    "\n",
    "# Directories for our training,\n",
    "# validation and test splits\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'val')\n",
    "test_dir = os.path.join(base_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "84396a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10354 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Resize all images to 150 X 150 (This is the parameter that we passed to our convnet)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir, # target directory\n",
    "        class_mode = 'categorical',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20) # Since we use binary_crossentropy loss, we need binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c734a9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2070 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_dir, # validation directory\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "48bbc688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch shape: (20, 320, 480, 3)\n",
      "labels batch shape: (20, 10)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6393b5e2",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "56128b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(\n",
    "    monitor='val_acc',\n",
    "    min_delta=0.001,\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e95d6416",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger('training.log', \n",
    "                       separator=',', \n",
    "                       append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb5d6a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 31s 281ms/step - loss: 2.2025 - acc: 0.2025 - val_loss: 2.0661 - val_acc: 0.2770\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 30s 296ms/step - loss: 1.8501 - acc: 0.3551 - val_loss: 1.7759 - val_acc: 0.4300\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 28s 280ms/step - loss: 1.6590 - acc: 0.4468 - val_loss: 1.7192 - val_acc: 0.4280\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 28s 279ms/step - loss: 1.5310 - acc: 0.5030 - val_loss: 1.5438 - val_acc: 0.5280\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 28s 279ms/step - loss: 1.4356 - acc: 0.5305 - val_loss: 1.4653 - val_acc: 0.5180\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 28s 280ms/step - loss: 1.3653 - acc: 0.5620 - val_loss: 1.3768 - val_acc: 0.5860\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 28s 278ms/step - loss: 1.2931 - acc: 0.5712 - val_loss: 1.3541 - val_acc: 0.5810\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 28s 279ms/step - loss: 1.2131 - acc: 0.6115 - val_loss: 1.3512 - val_acc: 0.5450\n",
      "Epoch 9/30\n",
      " 65/100 [==================>...........] - ETA: 7s - loss: 1.1797 - acc: 0.6208"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=30,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50,\n",
    "      callbacks=[earlystop, csv_logger]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097fa8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('simpson.h6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd98a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddc9972",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5763a311",
   "metadata": {},
   "source": [
    "Purpose: Has not been applied data augumentation, which could be applied due to a reduced data training sample that causes overfitting.\n",
    "By the latter graphs we can infer that we do not encounter overfitting, however, through transfer learning, we will keep the already trained weights and try to apply dropout and data aumentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "343ef3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras in ./.local/lib/python3.6/site-packages (2.6.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install keras --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d95526e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "27998e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import RandomFlip\n",
    "from keras.layers import RandomRotation\n",
    "from keras.layers import Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31169ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This has to be substituted with our model once we understand from the professor \n",
    "with what criterion the layers.trainable = False are selected '''\n",
    "\n",
    "base_model = keras.applications.ResNet50(\n",
    "    weights='imagenet',\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False,\n",
    ")\n",
    "\n",
    "# freeze the conv-net structure\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f51d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "       RandomFlip(\"horizontal\"),\n",
    "       RandomRotation(0.1),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8973961",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(150, 150, 3), name=\"input\")\n",
    "# Augment your inputs here!\n",
    "x = data_augmentation(inputs)\n",
    "\n",
    "# scale pixels here\n",
    "scale_layer = Rescaling(scale=1/127.5, offset=-1)\n",
    "x = scale_layer(x)\n",
    "\n",
    "x = base_model(x, training=False)               # Set it to inference mode\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)    # Pooling\n",
    "x = keras.layers.Dropout(0.4)(x)                # Regularize with dropout\n",
    "outputs = keras.layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49203e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.9),\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()],\n",
    ")\n",
    "\n",
    "epochs = 2\n",
    "model.fit(train_generator, epochs=epochs, validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80069fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS WILL TAKE A WHILE ON CPUs ###\n",
    "base_model.trainable = True\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-5),  # Low learning rate\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()],\n",
    ")\n",
    "\n",
    "epochs = 2\n",
    "model.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n",
    "### THIS WILL TAKE A WHILE ON CPUs ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b9693",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
